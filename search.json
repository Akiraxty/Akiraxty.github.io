[{"title":"计算机视觉知识点总结","date":"2022-07-07T10:20:24.678Z","url":"/2022/07/07/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/","tags":[["计算机视觉","/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"],["知识总结","/tags/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"]],"categories":[["计算机视觉","/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"]],"content":"计算机视觉知识点 idea Video Transformer 渐进式正则 余弦学习率变化曲线 多任务学习 自监督方法 推理速度 注意力可视化 TSM Nowadays，most of action recognition method lost the long distance relationship of frams. Most of method use a clip of frames , not entire video 用全连接给inception的每一个kernal大小一定的权重 双头的自注意力 四元数行为识别 trick Regulization Normalization Batch_Normalization：需要平移和缩放参数，具有减少前面输入都后面神经元的影响，且具有正则的作用，因为是每一个把它出门的均值和方差x’&#x3D;(1-momentum)x+momentumx’x′&#x3D;(1−momentum)∗x+momentum∗x′来更新下一个batch的均值和方差。mini_batch里面所有样本的某个通道的值作为一组，组内做Normalization，要求Batch大小比较大，，用滑动平均方法更新均值和方差，test阶段直接使用train阶段更新完之后的均值和方差。然后用 x′&#x3D;(1−momentum)∗x+momentum∗x′x’&#x3D;(1-momentum)x+momentumx’ Layer_Normalization 单个样本的指定维度上的数据做Normalization，一般用于NLP，RNN，因为seq的Batch大小往往不一样 Group_Normalization Dropout和Batch Nomalization不能一起使用，在train阶段dropout以p的比率丢掉几个神经元结果之后，为了保持与test阶段的均值差不多，会把其余值除以（1-p），均值变为1&#x2F;(1-p)1&#x2F;(1−p)倍，这时方差变为1&#x2F;(1-p)21&#x2F;(1−p)2倍，而test阶段由于是eval()模式，不进行dropout,均值和train阶段差不多，方差不同，但是Batch Normalization的均值和方差是使用train的，因此不匹配，结果拉跨。 1&#x2F;(1−p)1&#x2F;(1-p) 1&#x2F;(1−p)21&#x2F;(1-p)2 Stochastic Depth Dropout直接删除某一层的结果， 单个样本的所有通道分组，所有组之间做Normalization，当Batch大小比较小时用， Rand Augment Mixup 渐进式正则方法， 预训练模型载入部分权重 修改dataloader 注意力可视化方法、 分布式GPU并行训练梯度和BN的均值和方差在GPU之间传输torch.distributed.lanuch 动态学习率,cosin学习率变化曲线scheduler.step() Tensorboard可视化 cnn和transformer的对比1.卷积存在归纳偏见（ inductive biases）：局部连接性和平移不变性，无法获取长距离的依赖关系2.耗费计算资源 Temporal Fearure Temporal Convolutional Networks(TCN,2016,CVPR) 在seq上做卷积核空洞卷积， Temporal Relation Network (TRN,2018,ECCV)  Temporal Shift Module(TSM,2019,CVPR) 将部分通道前移到下一个下一帧，以在不增加计算量的情况下，获取时间特征，效果居然比I3D好，实验试一试 None-Local Module(NL,2017,CVPR)  Attention ECA SE CBAM None-Local Axial Attention Self-Attention Transformer Hybird Visual Transformer 使用ResNet之类的代替Transformer中的投影 Visual Transformer（ViT） 数据集比较大的时候才有点用，参数量比较小 Activation Function ReLU GeLU 为了增加模型的泛化能力，GELU引入随机正则，对输入乘以一个0，1组成的mask，mask服从伯努利分布，输出服从正态分布。效果很不错，下次实验试一试 评估方法 混淆矩阵：准确率、精确率、召回率、特异率 论文词汇 However, their costly memory access causes their actual latency to be signiﬁcantly larger than that of the convolutional networks [Local relation networks for image recognition]. 常见网络 EffcientNetV1&#x2F;V2 数理统计 概率是已知模型和参数，推数据。 统计是已知数据，推模型和参数。 最大似然估计（MLE）已知参数估计结果，使得似然函数P(x∣θ)最大 最大后验概率估计（MAP）是求参数θ，使得P(x∣θ)p(θ)最大。 先验概率p(θ) 评价指标 FLOPS每秒浮点运算次数 FLOPs浮点运算数，即计算量，GFLOPs表示10亿次浮点运算数。每一层卷积层的计算量公式如上图所示。 torchstat库from torchstat import statimport torchvision.models as modelsmodel &#x3D; model.alexnet()stat(model, (3, 224, 224)) 吞吐量库  pytorch-image-models 自监督 前置任务（Pretext task）：恢复被损毁的输入、去噪自编码、上下文自编码、跨通道自动编码（色彩化）、跟踪、分割、块排序、特征聚类。 对比学习（Contrastive learning）：InfoNCE(noise-contrastive estimation) 各种卷积操作 Transformer"},{"title":"Leecode刷题笔记","date":"2022-07-07T10:16:22.638Z","url":"/2022/07/07/Leecode%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/","tags":[["Leecode","/tags/Leecode/"]],"categories":[["Leecode","/categories/Leecode/"]],"content":"Leecode刷题遇到的一些不了解的函数以及知识点ps：使用的是python 哈希表： reduce() ord()：返回asci码 chr()：把ascii转为字母 itertools.product(a,b)：笛卡尔积 zip：打包为元组的列表 set()：创建不重复的元素集合[‘r’, ‘b’, ‘u’, ‘n’] “&#x2F;”，这是传统的除法，3&#x2F;2&#x3D;1.5 “&#x2F;&#x2F;”，在python中，这个叫“地板除”，3&#x2F;&#x2F;2&#x3D;1 “%”，这个是取模操作，也就是区余数，4%2&#x3D;0，5%2&#x3D;1"},{"title":"论文写作积累","date":"2022-04-28T16:47:43.113Z","url":"/2022/04/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%E7%A7%AF%E7%B4%AF/","tags":[["论文写作","/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"]],"categories":[["论文写作","/categories/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"]],"content":"英文缩写 i.e. 即 s.t. 服从，满足 ，受约束于 e.g. 例如 w.r.t. 相对于 et al. 等人 短语 is still very much up for debate：任然有待争论 orders of magnitude：数量级 vice versa：反之亦然 Pioneering works：以往工作 uncurated：非仔细筛选的 单词，句子收集 Substantial experiments 大量的实验 Most existing methods degenerate the applicability of models due to their intractable hyper-parameters triggered by various regularization terms. 现有的方法由于各种正则化项引起的超参数难以处理而降低了模型的适用性。 coalesces 结合 heterogeneous • 异类 • 异质的 • 多样化的 • 非均匀的 manifold 各种各样的 trivial solution：平凡解&#x2F; trivial task轻松的任务 prevalent 流行的，普遍的 Lemma 1 引理1 eigenvalue 特征值 materialized 具体化 deficiencies 不足，缺陷 canonical 规范化 dictates 规定 off-the-shelf 现成的 feasible 可行的 encapsulate 简要概括 We will elaborate the implementation in Section 4.3.5 We empirically demonstrate Figure 3 shows a schematic(概要的) overview of the model circumventing 失灵 Pitfalls 缺陷，陷阱 justifies 证明合理 This justifies their inclusion in the final contrastive loss referred to 称为 theoretically and empirically 理论和实际上的 To the best of our knowledge 据我们所知 versatile learning loss 全能的训练损失 endows 赋予 without loss of generality 不失一般性 namely 也就是 nonmonotonic 非单调的&#x2F;monotonously单调地 de-facto 实际上 Vice versa 反之同理 In this work, we are dedicated to "},{"title":"windows利用Vsocode和ssh登入堡垒机中的容器","date":"2022-03-09T15:55:02.144Z","url":"/2022/03/09/windows%E5%88%A9%E7%94%A8ssh%E7%99%BB%E5%85%A5%E5%A0%A1%E5%9E%92%E6%9C%BA%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8/","tags":[["Vscode","/tags/Vscode/"],["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"windows利用Vsocode和ssh登入堡垒机中的容器1 容器私钥&#x2F;堡垒机私钥 target.pem&#x2F;proxy.pem ps：Windos用户配置时，这两个钥匙最好放在C:&#x2F;User&#x2F;用户名&#x2F;里面，并且要进行如下设置，不然可能还要输入密码 右键文件属性-安全-高级-禁用继承-（选择第一个） 2 git bash设置私钥权限: chmod 600 target.pem&#x2F;chmod 600 proxy.pem 3 对容器 (container) 连线: 4 Vscode配置 "},{"title":"Mask R-CNN","date":"2022-03-05T07:43:15.070Z","url":"/2022/03/05/MaskedRCNN/","tags":[["论文笔记","/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"],["检测分割","/tags/%E6%A3%80%E6%B5%8B%E5%88%86%E5%89%B2/"]],"categories":[["论文笔记","/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"]],"content":"Mask R-CNN原文链接 IdeaR-CNN: Fast R-CNN： RoIPooling：就是给RoI特征分区，每一个区域单独做maxpooling Faster R-CNN： 输出: 类别标签 box偏移 步骤： Region Proposal Network (RPN)：选出候选边界框 使用PoIPool从box提取特征，然后进行分类和box回归 Masked R-CNN输出的时候加一个 输出 object mask 步骤： Region Proposal Network (RPN) Loss Function 词法分析 缩略词 i.e. 即 s.t. 服从，满足 ，受约束于 e.g. 例如 w.r.t. 相对于 et al. 等人 连接词 Moreover Without bells and whistles: 没有花哨的装饰 Formally Thus Specifically Next To address this, 语法分析 We present a conceptually simple, flexible, and generalframework for object instance segmentation. "}]